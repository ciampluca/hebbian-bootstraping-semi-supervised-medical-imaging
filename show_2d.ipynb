{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions\n",
    "\n",
    "def check_num_epochs(run):\n",
    "    if Path(run / \"val_log.csv\").is_file():\n",
    "        with open(run / 'config.json') as f:\n",
    "            cfg = json.load(f)\n",
    "\n",
    "        num_epochs = cfg['num_epochs']\n",
    "        val_iter = cfg['validate_iter']\n",
    "        num_epochs = int(num_epochs / val_iter)\n",
    "\n",
    "        valid_log = pd.read_csv(run / \"val_log.csv\", header=None, index_col=0)\n",
    "        if (len(valid_log.index)-1) < num_epochs:\n",
    "            print(\"Wrong number of epochs in run: {}\".format(run))\n",
    "    else:\n",
    "        print(\"val_log.csv not exists in run: {}\".format(run))\n",
    "\n",
    "def check_only_one_tensorboard(run):\n",
    "    if len(list(Path(run / \"runs\").glob('*'))) > 1:\n",
    "        print(\"More than 1 tensorboard folder in run: {}\".format(run))\n",
    "\n",
    "def collect_one(model_name, run, csv_file):\n",
    "    check_num_epochs(run)\n",
    "    check_only_one_tensorboard(run)\n",
    "    \n",
    "    with open(run / 'config.json') as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    run_number = cfg['seed']\n",
    "    regime, inv_temp = float(run.parent.parts[-1].rsplit('-', 1)[1]), float(run.parent.parts[-2].rsplit('-', 1)[1])\n",
    "\n",
    "    csv_path = run / csv_file\n",
    "    if not csv_path.exists():\n",
    "        print(f'Skipping not found: {csv_path}')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = pd.read_csv(csv_path)\n",
    "    if data.empty:\n",
    "        print(f'Pred file is empty: {csv_path}')\n",
    "\n",
    "    data['model'] = model_name\n",
    "    data['run_number'] = int(run_number)\n",
    "    data['inv_temp'] = int(inv_temp)\n",
    "    data['regime'] = int(regime)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def collect_all(model_name, root, csv_file, regimes=['1', '2', '5', '10', '20','100'], ignore_outliers=True):\n",
    "    root = Path(root)\n",
    "\n",
    "    metrics = []\n",
    "    for inv_temp in list(root.glob(\"inv_temp-*\")):\n",
    "        for regime in list(inv_temp.glob(\"regime-*\")):\n",
    "            if regime.name.rsplit(\"-\", 1)[1] in regimes:\n",
    "                for run in list(regime.glob(\"run-*\")):\n",
    "                    if ignore_outliers and 'outlier' in run.as_posix():\n",
    "                        continue\n",
    "                    else:\n",
    "                        metrics.append(collect_one(model_name, run, csv_file))\n",
    "        \n",
    "    metrics = pd.concat(metrics, ignore_index=True)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for each detected run\n",
    "\n",
    "def compute_metrics(data, grouping, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance']):\n",
    "    metrics_dict_names = {\n",
    "        'Dice': 'dice',\n",
    "        'Jaccard': 'jaccard',\n",
    "        'Hausdorff Distance': '95hd',\n",
    "        'Average Surface Distance': 'asd'\n",
    "    }\n",
    "\n",
    "    columns = ['Model', '# Run', 'Inv Temp', 'Regime']\n",
    "    columns.extend(metric_names)\n",
    "    metrics = []\n",
    "    \n",
    "    data = data.copy().reset_index()\n",
    "    grouped = data.groupby(grouping)\n",
    "    \n",
    "    for model_group, predictions in grouped:\n",
    "        model_name, run_number, inv_temp, regime = model_group[0], int(model_group[1]), int(model_group[2]), int(model_group[3])\n",
    "        \n",
    "        metric_values = []\n",
    "        for metric_name in metric_names:\n",
    "            values = predictions['segm/{}'.format(metrics_dict_names[metric_name])].values\n",
    "            mean_value = np.nanmean(values)\n",
    "            if metric_name == 'Dice' or metric_name == 'Jaccard':\n",
    "                metric_values.append((math.ceil(mean_value*10000)/10000)*100)\n",
    "            else:\n",
    "                if not np.isnan(mean_value):\n",
    "                    metric_values.append(math.ceil(mean_value*100)/100)\n",
    "                else:\n",
    "                    metric_values.append(mean_value)\n",
    "        \n",
    "        metrics.append([model_name, run_number, inv_temp, regime, *metric_values])\n",
    "        \n",
    "    metrics_df = pd.DataFrame(metrics, columns=columns)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def summarize_metrics(metrics, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'], confidence_level=0.90, return_ranges=False):\n",
    "\n",
    "    def compute_ci(values, return_ranges=False):\n",
    "        ci = st.t.interval(confidence_level, len(values)-1, loc=np.mean(values), scale=np.std(values)/(len(values)**0.5))\n",
    "\n",
    "        if return_ranges:\n",
    "            return ci\n",
    "        else:\n",
    "            return (ci[1]-ci[0]) / 2\n",
    "\n",
    "    #mean_metrics = metrics.groupby(['Model', 'Inv Temp', 'Regime'])[metric_names].aggregate([('Mean', np.mean), ('STD', np.std), (\"CI {}%\".format(confidence_level), compute_ci)])\n",
    "    mean_metrics = metrics.groupby(['Model', 'Inv Temp', 'Regime'])[metric_names].aggregate([('Mean', 'mean'), (\"CI {}%\".format(confidence_level), compute_ci)]) \n",
    "\n",
    "    return mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GlaS Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation - Searching temperature hyperparameter</h2>\n",
    "\n",
    "<p>Evaluate Hebbian models belonging to SWTA paradigm to search best temperature values (this value is dataset-specific)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ROOT = \"./runs\"\n",
    "\n",
    "REGIMES = ['100']\n",
    "\n",
    "runs = {\n",
    "    'H-UNet-SWTA-T': list(Path(EXP_ROOT + '/GlaS/hebbian_unsup/').glob('unet_swta_t')),\n",
    "    'H-UNet-URPC-SWTA-T': list(Path(EXP_ROOT + '/GlaS/hebbian_unsup/').glob('unet_urpc_swta_t')),\n",
    "    'H-UNet-CCT-SWTA-T': list(Path(EXP_ROOT + '/GlaS/hebbian_unsup/').glob('unet_cct_swta_t')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions scanning runs\n",
    "predictions = pd.concat([collect_all(k, r, 'test.csv', regimes=REGIMES, ignore_outliers=False) for k, v in runs.items() for r in v], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics\n",
    "model_grouper = ['model', 'run_number', 'inv_temp', 'regime']\n",
    "metrics = compute_metrics(predictions, model_grouper, metric_names=['Dice', 'Jaccard'])\n",
    "\n",
    "display(metrics)\n",
    "\n",
    "summary = summarize_metrics(metrics, metric_names=['Dice', 'Jaccard'])\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ROOT = \"./runs\"\n",
    "\n",
    "REGIMES = ['1', '2', '5', '10', '20']\n",
    "\n",
    "runs = {\n",
    "    'H-UNet-SWTA-T': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('h_unet_swta_t')),\n",
    "    'H-EM-SWTA-T': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('h_em_unet_swta_t')),\n",
    "    'H-UAMT-SWTA-T': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('h_uamt_unet_swta_t')),\n",
    "    'H-CPS-SWTA-T': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('h_cps_unet_swta_t')),\n",
    "    'H-URPC-SWTA-T': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('h_urpc_unet_swta_t')),\n",
    "    'H-CCT-SWTA-T': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('h_cct_unet_swta_t')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions scanning runs\n",
    "predictions = pd.concat([collect_all(k, r, 'test.csv', regimes=REGIMES, ignore_outliers=False) for k, v in runs.items() for r in v], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics\n",
    "model_grouper = ['model', 'run_number', 'inv_temp', 'regime']\n",
    "metrics = compute_metrics(predictions, model_grouper, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'])\n",
    "\n",
    "display(metrics)\n",
    "\n",
    "summary = summarize_metrics(metrics, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'], confidence_level=0.90)\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation - Weight init comparison</h2>\n",
    "\n",
    "<p>Evaluate weight init methods (only for baseline)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ROOT = \"./runs\"\n",
    "\n",
    "REGIMES = ['1', '2', '5', '10', '20']\n",
    "\n",
    "runs = {\n",
    "    'Kaiming-UNet': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('kaiming_unet')),\n",
    "    #'Xavier-UNet': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('xavier_unet')),\n",
    "    #'Orthogonal-UNet': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('orthogonal_unet')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions scanning runs\n",
    "predictions = pd.concat([collect_all(k, r, 'test.csv', regimes=REGIMES, ignore_outliers=False) for k, v in runs.items() for r in v], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grouper = ['model', 'run_number', 'inv_temp', 'regime']\n",
    "metrics = compute_metrics(predictions, model_grouper, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'])\n",
    "\n",
    "display(metrics)\n",
    "\n",
    "summary = summarize_metrics(metrics, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'], confidence_level=0.90)\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation - Data regime variations</h2>\n",
    "\n",
    "<p>Evaluate Hebbian models over the datasets, by varying the quantity of training data; only fine-tuned models are considered since during pre-training we can consider the whole dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ROOT = \"./runs\"\n",
    "\n",
    "REGIMES = ['1', '2', '5', '10', '20']\n",
    "\n",
    "runs = {\n",
    "    'UNet': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('kaiming_unet')),\n",
    "    'EM': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('em_unet')),\n",
    "    'UAMT': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('uamt_unet')),\n",
    "    'CPS': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('cps_unet')),\n",
    "    'URPC': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('urpc_unet')),\n",
    "    'CCT': list(Path(EXP_ROOT + '/GlaS/semi_sup/').glob('cct_unet')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions scanning runs\n",
    "predictions = pd.concat([collect_all(k, r, 'test.csv', regimes=REGIMES, ignore_outliers=False) for k, v in runs.items() for r in v], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grouper = ['model', 'run_number', 'inv_temp', 'regime']\n",
    "metrics = compute_metrics(predictions, model_grouper, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'])\n",
    "\n",
    "display(metrics)\n",
    "\n",
    "summary = summarize_metrics(metrics, metric_names=['Dice', 'Jaccard', 'Hausdorff Distance', 'Average Surface Distance'], confidence_level=0.90)\n",
    "\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
